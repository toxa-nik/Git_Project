{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aca0353b",
      "metadata": {
        "id": "aca0353b"
      },
      "source": [
        "# Домашнее задание 3. Парсинг, Git и тестирование на Python\n",
        "\n",
        "**Цели задания:**\n",
        "\n",
        "* Освоить базовые подходы к web-scraping с библиотеками `requests` и `BeautisulSoup`: навигация по страницам, извлечение HTML-элементов, парсинг.\n",
        "* Научиться автоматизировать задачи с использованием библиотеки `schedule`.\n",
        "* Попрактиковаться в использовании Git и оформлении проектов на GitHub.\n",
        "* Написать и запустить простые юнит-тесты с использованием `pytest`.\n",
        "\n",
        "\n",
        "В этом домашнем задании вы разработаете систему для автоматического сбора данных о книгах с сайта [Books to Scrape](http://books.toscrape.com). Нужно реализовать функции для парсинга всех страниц сайта, извлечения информации о книгах, автоматического ежедневного запуска задачи и сохранения результата.\n",
        "\n",
        "Важной частью задания станет оформление проекта: вы создадите репозиторий на GitHub, оформите `README.md`, добавите артефакты (код, данные, отчеты) и напишете базовые тесты на `pytest`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "K3JMV0qwmA_q",
      "metadata": {
        "id": "K3JMV0qwmA_q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in c:\\users\\toxan\\anaconda3\\lib\\site-packages (2.32.3)\n",
            "Requirement already satisfied: schedule in c:\\users\\toxan\\anaconda3\\lib\\site-packages (1.2.2)\n",
            "Requirement already satisfied: BeautifulSoup4 in c:\\users\\toxan\\anaconda3\\lib\\site-packages (4.12.3)\n",
            "Requirement already satisfied: pytest in c:\\users\\toxan\\anaconda3\\lib\\site-packages (7.4.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\toxan\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\toxan\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\toxan\\anaconda3\\lib\\site-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\toxan\\anaconda3\\lib\\site-packages (from requests) (2025.7.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\toxan\\anaconda3\\lib\\site-packages (from BeautifulSoup4) (2.5)\n",
            "Requirement already satisfied: iniconfig in c:\\users\\toxan\\anaconda3\\lib\\site-packages (from pytest) (1.1.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\toxan\\anaconda3\\lib\\site-packages (from pytest) (24.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\toxan\\anaconda3\\lib\\site-packages (from pytest) (1.0.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\toxan\\anaconda3\\lib\\site-packages (from pytest) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "! pip install requests schedule BeautifulSoup4 pytest "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "873d4904",
      "metadata": {
        "id": "873d4904"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Все зависимости готовы!\n"
          ]
        }
      ],
      "source": [
        "# Библиотеки, которые могут вам понадобиться\n",
        "# При необходимости расширяйте список\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    import requests\n",
        "    import schedule\n",
        "    from bs4 import BeautifulSoup\n",
        "    import pytest\n",
        "\n",
        "    print(\"Все зависимости готовы!\")\n",
        "except ImportError as e:\n",
        "    print(f\"Ошибка импорта: {e}\")\n",
        "    print(\"Установите зависимости: pip install -r requirements.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "unTvsWaegHdj",
      "metadata": {
        "id": "unTvsWaegHdj"
      },
      "source": [
        "## Задание 1. Сбор данных об одной книге (20 баллов)\n",
        "\n",
        "В этом задании мы начнем подготовку скрипта для парсинга информации о книгах со страниц каталога сайта [Books to Scrape](https://books.toscrape.com/).\n",
        "\n",
        "Для начала реализуйте функцию `get_book_data`, которая будет получать данные о книге с одной страницы (например, с [этой](http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html)). Соберите всю информацию, включая название, цену, рейтинг, количество в наличии, описание и дополнительные характеристики из таблицы Product Information. Результат достаточно вернуть в виде словаря.\n",
        "\n",
        "**Не забывайте про соблюдение PEP-8** — помимо качественно написанного кода важно также документировать функции по стандарту:\n",
        "* кратко описать, что она делает и для чего нужна;\n",
        "* какие входные аргументы принимает, какого они типа и что означают по смыслу;\n",
        "* аналогично описать возвращаемые значения.\n",
        "\n",
        "*P. S. Состав, количество аргументов функции и тип возвращаемого значения можете менять как вам удобно. То, что написано ниже в шаблоне — лишь пример.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "UfD2vAjHkEoS",
      "metadata": {
        "id": "UfD2vAjHkEoS"
      },
      "outputs": [],
      "source": [
        "def get_book_data(book_url: str) -> dict:\n",
        "    \"\"\"\n",
        "    Парсит данные о книге с указанного Url.\n",
        "\n",
        "    Функция загружает HTML-страницу книги и извлекает информацию.\n",
        "\n",
        "    Args:\n",
        "        book_url (str): URL-адрес страницы книги для парсинга\n",
        "\n",
        "    Returns:\n",
        "        dict: Словарь с данными о книге, содержащий следующие ключи:\n",
        "            - Name (str): Название книги\n",
        "            - Rating (str): Рейтинг от 1 до 5\n",
        "            - Description (str): Описание книги или \"No description available\"\n",
        "            - UPC (str): Код книги\n",
        "            - Product Type (str): Тип продукта\n",
        "            - Price (excl. tax) (str): Цена без налога\n",
        "            - Price (incl. tax) (str): Цена с налогом\n",
        "            - Tax (str): Размер налога\n",
        "            - Availability (str): Информация о наличии\n",
        "            - Number of reviews (str): Количество отзывов\n",
        "\n",
        "    Raises:\n",
        "        requests.RequestException: В случае ошибки сетевого запроса\n",
        "\n",
        "    Note:\n",
        "        В случае ошибки при загрузке страницы возвращает пустой словарь {}\n",
        "        и выводит сообщение об ошибке.\n",
        "    \"\"\"\n",
        "\n",
        "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "    session = requests.Session()\n",
        "\n",
        "    try:\n",
        "        page = session.get(book_url)\n",
        "        page.raise_for_status()\n",
        "        page.encoding = \"utf-8\"\n",
        "        soup = BeautifulSoup(page.text, \"html.parser\")\n",
        "\n",
        "        product_dict = {}\n",
        "\n",
        "        name = soup.find(\"div\", class_=\"col-sm-6 product_main\").find(\"h1\").get_text()\n",
        "        product_dict[\"Name\"] = name\n",
        "\n",
        "        stars = (\n",
        "            soup.find(\"p\", class_=\"star-rating\")\n",
        "            .get(\"class\")[1]\n",
        "            .replace(\"One\", \"1\")\n",
        "            .replace(\"Two\", \"2\")\n",
        "            .replace(\"Three\", \"3\")\n",
        "            .replace(\"Four\", \"4\")\n",
        "            .replace(\"Five\", \"5\")\n",
        "        )\n",
        "        product_dict[\"Rating\"] = stars\n",
        "\n",
        "        description_element = soup.find(\n",
        "            \"div\", id=\"product_description\", class_=\"sub-header\"\n",
        "        )\n",
        "        if description_element and description_element.find_next_sibling():\n",
        "            description = (\n",
        "                soup.find(\"div\", id=\"product_description\", class_=\"sub-header\")\n",
        "                .find_next_sibling()\n",
        "                .get_text()\n",
        "                .replace(\"\\xa0\", \"\")\n",
        "            )\n",
        "            product_dict[\"Description\"] = description\n",
        "        else:\n",
        "            product_dict[\"Description\"] = \"No description available\"\n",
        "\n",
        "        table = soup.find(\"table\", class_=\"table table-striped\").find_all(\"tr\")\n",
        "        for row in table:\n",
        "            product_dict[row.find(\"th\").get_text().strip()] = (\n",
        "                row.find(\"td\").get_text().strip()\n",
        "            )\n",
        "\n",
        "        return product_dict\n",
        "\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Ошибка при загрузке страницы: {e}\")\n",
        "        return {}\n",
        "\n",
        "    finally:\n",
        "        session.close()\n",
        "# КОНЕЦ ВАШЕГО РЕШЕНИЯ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "moRSO9Itp1LT",
      "metadata": {
        "id": "moRSO9Itp1LT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Name': 'A Light in the Attic', 'Rating': '3', 'Description': \"It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love th It's hard to imagine a world without A Light in the Attic. This now-classic collection of poetry and drawings from Shel Silverstein celebrates its 20th anniversary with this special edition. Silverstein's humorous and creative verse can amuse the dowdiest of readers. Lemon-faced adults and fidgety kids sit still and read these rhythmic words and laugh and smile and love that Silverstein. Need proof of his genius? RockabyeRockabye baby, in the treetopDon't you know a treetopIs no safe place to rock?And who put you up there,And your cradle, too?Baby, I think someone down here'sGot it in for you. Shel, you never sounded so good. ...more\", 'UPC': 'a897fe39b1053632', 'Product Type': 'Books', 'Price (excl. tax)': '£51.77', 'Price (incl. tax)': '£51.77', 'Tax': '£0.00', 'Availability': 'In stock (22 available)', 'Number of reviews': '0'}\n"
          ]
        }
      ],
      "source": [
        "# Используйте для самопроверки\n",
        "book_url = \"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\"\n",
        "print(get_book_data(book_url))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u601Q4evosq6",
      "metadata": {
        "id": "u601Q4evosq6"
      },
      "source": [
        "## Задание 2. Сбор данных обо всех книгах (20 баллов)\n",
        "\n",
        "Создайте функцию `scrape_books`, которая будет проходиться по всем страницам из каталога (вида `http://books.toscrape.com/catalogue/page-{N}.html`) и осуществлять парсинг всех страниц в цикле, используя ранее написанную `get_book_data`.\n",
        "\n",
        "Добавьте аргумент-флаг, который будет отвечать за сохранение результата в файл: если он будет равен `True`, то информация сохранится в ту же папку в файл `books_data.txt`; иначе шаг сохранения будет пропущен.\n",
        "\n",
        "**Также не забывайте про соблюдение PEP-8**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "kk78l6oDkdxl",
      "metadata": {
        "id": "kk78l6oDkdxl"
      },
      "outputs": [],
      "source": [
        "def _get_url_list(catalog_url: str, page_count: int = 0) -> list:\n",
        "    \"\"\"\n",
        "    Генерирует список URL всех книг из каталога.\n",
        "    Внутренняя вспомогательная функция для извлечения ссылок на книги\n",
        "    со страниц каталога.\n",
        "\n",
        "    Args:\n",
        "        catalog_url (str): URL начальной страницы каталога\n",
        "        page_count (int, optional): Количество страниц для парсинга.\n",
        "            При значении 0 обрабатываются все страницы. По умолчанию 0.\n",
        "\n",
        "    Returns:\n",
        "        list: Список абсолютных URL отдельных книг\n",
        "\n",
        "    Raises:\n",
        "        requests.RequestException: В случае ошибки сетевого запроса\n",
        "\n",
        "    Note:\n",
        "        Функция выводит прогресс обработки в консоль и измеряет время выполнения.\n",
        "        При отрицательном page_count возвращает пустой список.\n",
        "    \"\"\"\n",
        "\n",
        "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "    get_url_list_start_time = time.time()\n",
        "    session = requests.Session()\n",
        "    big_list = []\n",
        "\n",
        "    try:\n",
        "        if page_count < 0:\n",
        "            print(f\"Введено отрицательное число страниц: {page_count}\")\n",
        "            return big_list\n",
        "\n",
        "        page_url = catalog_url\n",
        "        upper_page_url = \"/\".join(catalog_url.split(\"/\")[:-1])\n",
        "        pages_processed = 0\n",
        "        max_pages = None\n",
        "\n",
        "        while True:\n",
        "            current_page = session.get(page_url)\n",
        "            current_page.raise_for_status()\n",
        "            current_page.encoding = \"utf-8\"\n",
        "            soup = BeautifulSoup(current_page.text, \"html.parser\")\n",
        "\n",
        "            current_page_numb = int(\n",
        "                soup.find(\"li\", class_=\"current\").get_text().split()[1]\n",
        "            )\n",
        "\n",
        "            if max_pages is None:\n",
        "                max_pages = int(\n",
        "                    soup.find(\"li\", class_=\"current\").get_text().split()[-1]\n",
        "                )\n",
        "                if page_count == 0:\n",
        "                    page_count = max_pages\n",
        "                else:\n",
        "                    page_count = min(page_count, max_pages)\n",
        "\n",
        "            current_page_link_list = []\n",
        "            links = soup.find_all(\"a\", title=True)\n",
        "            for link in links:\n",
        "                href = link.get(\"href\")\n",
        "                if href:\n",
        "                    absolute_url = upper_page_url + \"/\" + href\n",
        "                    current_page_link_list.append(absolute_url)\n",
        "\n",
        "            big_list.extend(current_page_link_list)  ########\n",
        "            pages_processed += 1\n",
        "            print(f\"Обработаны ссылки со страницы №{current_page_numb}\")\n",
        "\n",
        "            if pages_processed >= page_count:\n",
        "                print(f\"Достигнуто заданное количество страниц: {page_count}\")\n",
        "                break\n",
        "\n",
        "            if current_page_numb >= max_pages:\n",
        "                print(\"Достигнута последняя страница каталога\")\n",
        "                break\n",
        "\n",
        "            page_url = re.sub(r\"\\d+\", str(int(current_page_numb) + 1), page_url)\n",
        "\n",
        "        get_url_list_end_time = time.time()\n",
        "        get_url_list_execution_time = get_url_list_end_time - get_url_list_start_time\n",
        "        print(f\"Время обработки ссылок: {round(get_url_list_execution_time, 2)} сек.\")\n",
        "\n",
        "        return big_list\n",
        "\n",
        "    finally:\n",
        "        session.close()\n",
        "\n",
        "\n",
        "def scrape_books(\n",
        "    catalog_url: str,\n",
        "    is_save: bool = False,\n",
        "    return_json: bool = True,\n",
        "    page_count: int = 0,\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Парсит данные о книгах из каталога.\n",
        "\n",
        "    Основная функция для сбора данных о книгах. Использует многопоточность\n",
        "    для параллельного парсинга страниц книг. Поддерживает сохранение результатов\n",
        "    в файл и различные форматы вывода.\n",
        "\n",
        "    Args:\n",
        "        catalog_url (str): URL каталога книг\n",
        "        is_save (bool, optional): Сохранять ли результат в файл.\n",
        "            По умолчанию False.\n",
        "        return_json (bool, optional): Возвращать результат в формате JSON.\n",
        "            При False возвращает список словарей. По умолчанию True.\n",
        "        page_count (int, optional): Количество страниц для парсинга.\n",
        "            При значении 0 обрабатываются все страницы. По умолчанию 0.\n",
        "\n",
        "    Returns:\n",
        "        list or str: Данные о книгах. При return_json=True возвращает JSON-строку,\n",
        "            при return_json=False возвращает список словарей.\n",
        "\n",
        "    Raises:\n",
        "        requests.RequestException: В случае ошибки сетевого запроса\n",
        "        Exception: В случае других ошибок при парсинге\n",
        "\n",
        "    Note:\n",
        "        Функция выводит прогресс выполнения и время работы в консоль.\n",
        "        При is_save=True создает файл 'books_data.txt' в папке artifacts.\n",
        "    \"\"\"\n",
        "\n",
        "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "    scrape_books_start_time = time.time()\n",
        "\n",
        "    url_list = _get_url_list(catalog_url, page_count)\n",
        "    print(f\"Всего найдено URL книг для парсинга: {len(url_list)}\")\n",
        "    print(\"Начинаю парсинг\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        results = executor.map(get_book_data, url_list)\n",
        "        big_data = [book_data for book_data in results if book_data]\n",
        "\n",
        "    if return_json:\n",
        "        big_data_json = json.dumps(big_data, ensure_ascii=False, indent=2)\n",
        "\n",
        "    if is_save:\n",
        "        script_dir = Path.cwd()\n",
        "        artifacts_dir = (\n",
        "            script_dir.parent / \"artifacts\"\n",
        "        )  # Изменить для скрипта на /\"artifacts\" и проверить обязательно!!!\n",
        "        file_path = artifacts_dir / \"books_data.txt\"\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            if return_json:\n",
        "                f.write(big_data_json)\n",
        "            else:\n",
        "                for line in big_data:\n",
        "                    f.write(str(line) + \"\\n\\n\")\n",
        "\n",
        "    scrape_books_end_time = time.time()\n",
        "    scrape_books_execution_time = scrape_books_end_time - scrape_books_start_time\n",
        "    print(f\"Время парсинга книг: {round(scrape_books_execution_time, 2)} сек.\")\n",
        "    print(f\"Обработано книг: {len(big_data)}\")\n",
        "    print(f\"Общее время работы: {round(scrape_books_execution_time, 2)} сек.\")\n",
        "\n",
        "    min_execution_time = 0.01\n",
        "    effective_time = max(scrape_books_execution_time, min_execution_time)\n",
        "    print(f\"Средняя скорость: {len(big_data) / effective_time:.2f} книг/сек\")\n",
        "\n",
        "    if return_json:\n",
        "        return big_data_json\n",
        "    else:\n",
        "        return big_data\n",
        "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "Bt7mrXcbkj5Q",
      "metadata": {
        "id": "Bt7mrXcbkj5Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Обработаны ссылки со страницы №1\n",
            "Обработаны ссылки со страницы №2\n",
            "Обработаны ссылки со страницы №3\n",
            "Обработаны ссылки со страницы №4\n",
            "Обработаны ссылки со страницы №5\n",
            "Обработаны ссылки со страницы №6\n",
            "Обработаны ссылки со страницы №7\n",
            "Обработаны ссылки со страницы №8\n",
            "Обработаны ссылки со страницы №9\n",
            "Обработаны ссылки со страницы №10\n",
            "Обработаны ссылки со страницы №11\n",
            "Обработаны ссылки со страницы №12\n",
            "Обработаны ссылки со страницы №13\n",
            "Обработаны ссылки со страницы №14\n",
            "Обработаны ссылки со страницы №15\n",
            "Обработаны ссылки со страницы №16\n",
            "Обработаны ссылки со страницы №17\n",
            "Обработаны ссылки со страницы №18\n",
            "Обработаны ссылки со страницы №19\n",
            "Обработаны ссылки со страницы №20\n",
            "Обработаны ссылки со страницы №21\n",
            "Обработаны ссылки со страницы №22\n",
            "Обработаны ссылки со страницы №23\n",
            "Обработаны ссылки со страницы №24\n",
            "Обработаны ссылки со страницы №25\n",
            "Обработаны ссылки со страницы №26\n",
            "Обработаны ссылки со страницы №27\n",
            "Обработаны ссылки со страницы №28\n",
            "Обработаны ссылки со страницы №29\n",
            "Обработаны ссылки со страницы №30\n",
            "Обработаны ссылки со страницы №31\n",
            "Обработаны ссылки со страницы №32\n",
            "Обработаны ссылки со страницы №33\n",
            "Обработаны ссылки со страницы №34\n",
            "Обработаны ссылки со страницы №35\n",
            "Обработаны ссылки со страницы №36\n",
            "Обработаны ссылки со страницы №37\n",
            "Обработаны ссылки со страницы №38\n",
            "Обработаны ссылки со страницы №39\n",
            "Обработаны ссылки со страницы №40\n",
            "Обработаны ссылки со страницы №41\n",
            "Обработаны ссылки со страницы №42\n",
            "Обработаны ссылки со страницы №43\n",
            "Обработаны ссылки со страницы №44\n",
            "Обработаны ссылки со страницы №45\n",
            "Обработаны ссылки со страницы №46\n",
            "Обработаны ссылки со страницы №47\n",
            "Обработаны ссылки со страницы №48\n",
            "Обработаны ссылки со страницы №49\n",
            "Обработаны ссылки со страницы №50\n",
            "Достигнуто заданное количество страниц: 50\n",
            "Время обработки ссылок: 9.91 сек.\n",
            "Всего найдено URL книг для парсинга: 1000\n",
            "Начинаю парсинг\n",
            "Время парсинга книг: 47.57 сек.\n",
            "<class 'str'> 1763531\n"
          ]
        }
      ],
      "source": [
        "# Проверка работоспособности функции\n",
        "catalog_url = 'http://books.toscrape.com/catalogue/page-1.html'\n",
        "res = scrape_books(catalog_url, is_save=True, return_json=True, page_count = 0) # Допишите ваши аргументы\n",
        "print(type(res), len(res)) # и проверки (проверки указал в функции)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z5fd728nl8a8",
      "metadata": {
        "id": "z5fd728nl8a8"
      },
      "source": [
        "## Задание 3. Настройка регулярной выгрузки (10 баллов)\n",
        "\n",
        "Настройте автоматический запуск функции сбора данных каждый день в 19:00.\n",
        "Для автоматизации используйте библиотеку `schedule`. Функция должна запускаться в указанное время и сохранять обновленные данные в текстовый файл.\n",
        "\n",
        "\n",
        "\n",
        "Бесконечный цикл должен обеспечивать постоянное ожидание времени для запуска задачи и выполнять ее по расписанию. Однако чтобы не перегружать систему, стоит подумать о том, чтобы выполнять проверку нужного времени не постоянно, а раз в какой-то промежуток. В этом вам может помочь `time.sleep(...)`.\n",
        "\n",
        "Проверьте работоспособность кода локально на любом времени чч:мм.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "SajRRCj4n8BZ",
      "metadata": {
        "id": "SajRRCj4n8BZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Запущен планировщик.\n",
            "Обработаны ссылки со страницы №1\n",
            "Обработаны ссылки со страницы №2\n",
            "Обработаны ссылки со страницы №3\n",
            "Обработаны ссылки со страницы №4\n",
            "Обработаны ссылки со страницы №5\n",
            "Достигнуто заданное количество страниц: 5\n",
            "Время обработки ссылок: 1.39 сек.\n",
            "Всего найдено URL книг для парсинга: 100\n",
            "Начинаю парсинг\n",
            "Время парсинга книг: 5.32 сек.\n",
            "Обработано книг: 100\n",
            "Общее время работы: 5.32 сек.\n",
            "Средняя скорость: 18.80 книг/сек\n",
            "\n",
            "Завершение ожидания. Программа остановлена вручную.\n"
          ]
        }
      ],
      "source": [
        "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
        "catalog_url = \"http://books.toscrape.com/catalogue/page-1.html\"\n",
        "schedule.every().day.at(\"13:49\").do(\n",
        "    scrape_books,\n",
        "    catalog_url,\n",
        "    is_save=True,\n",
        "    return_json=True,\n",
        "    page_count = 5,\n",
        ")\n",
        "\n",
        "print(\"Запущен планировщик.\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        schedule.run_pending()\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nЗавершение ожидания. Программа остановлена вручную.\")\n",
        "# КОНЕЦ ВАШЕГО РЕШЕНИЯ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XFiPtEyaoLxq",
      "metadata": {
        "id": "XFiPtEyaoLxq"
      },
      "source": [
        "## Задание 4. Написание автотестов (15 баллов)\n",
        "\n",
        "Создайте минимум три автотеста для ключевых функций парсинга — например, `get_book_data` и `scrape_books`. Идеи проверок (можете использовать свои):\n",
        "\n",
        "* данные о книге возвращаются в виде словаря с нужными ключами;\n",
        "* список ссылок или количество собранных книг соответствует ожиданиям;\n",
        "* значения отдельных полей (например, `title`) корректны.\n",
        "\n",
        "Оформите тесты в отдельном скрипте `tests/test_scraper.py`, используйте библиотеку `pytest`. Убедитесь, что тесты проходят успешно при запуске из терминала командой `pytest`.\n",
        "\n",
        "Также выведите результат их выполнения в ячейке ниже.\n",
        "\n",
        "**Не забывайте про соблюдение PEP-8**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "lBFAw4b3z8QY",
      "metadata": {
        "id": "lBFAw4b3z8QY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m============================= test session starts =============================\u001b[0m\n",
            "platform win32 -- Python 3.12.7, pytest-7.4.4, pluggy-1.0.0 -- C:\\Users\\toxan\\anaconda3\\python.exe\n",
            "cachedir: .pytest_cache\n",
            "rootdir: c:\\Users\\toxan\\Desktop\\Учёба\\Homework_3\\Git_Project\n",
            "plugins: anyio-4.2.0\n",
            "\u001b[1mcollecting ... \u001b[0mcollected 7 items\n",
            "\n",
            "tests/test_scraper.py::TestGetBookData::test_returns_dict_with_required_keys \u001b[32mPASSED\u001b[0m\u001b[32m [ 14%]\u001b[0m\n",
            "tests/test_scraper.py::TestGetBookData::test_book_title_not_empty \u001b[32mPASSED\u001b[0m\u001b[32m [ 28%]\u001b[0m\n",
            "tests/test_scraper.py::TestGetBookData::test_rating_is_valid \u001b[32mPASSED\u001b[0m\u001b[32m      [ 42%]\u001b[0m\n",
            "tests/test_scraper.py::TestScrapeBooks::test_returns_list_of_books \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n",
            "tests/test_scraper.py::TestScrapeBooks::test_books_have_required_fields \u001b[32mPASSED\u001b[0m\u001b[32m [ 71%]\u001b[0m\n",
            "tests/test_scraper.py::TestScrapeBooks::test_json_output_format \u001b[32mPASSED\u001b[0m\u001b[32m   [ 85%]\u001b[0m\n",
            "tests/test_scraper.py::test_bad_page_count_handling \u001b[32mPASSED\u001b[0m\u001b[32m               [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================== \u001b[32m\u001b[1m7 passed\u001b[0m\u001b[32m in 5.39s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Ячейка для демонстрации работоспособности\n",
        "# Сам код напишите в отдельном скрипте\n",
        "! cd .. && pytest tests/test_scraper.py -v\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cRSQlHfRtOdN",
      "metadata": {
        "id": "cRSQlHfRtOdN"
      },
      "source": [
        "## Задание 5. Оформление проекта на GitHub и работа с Git (35 баллов)\n",
        "\n",
        "В этом задании нужно воспользоваться системой контроля версий Git и платформой GitHub для хранения и управления своим проектом. **Ссылку на свой репозиторий пришлите в форме для сдачи ответа.**\n",
        "\n",
        "### Пошаговая инструкция и задания\n",
        "\n",
        "**1. Установите Git на свой компьютер.**\n",
        "\n",
        "* Для Windows: [скачайте установщик](https://git-scm.com/downloads) и выполните установку.\n",
        "* Для macOS:\n",
        "\n",
        "  ```\n",
        "  brew install git\n",
        "  ```\n",
        "* Для Linux:\n",
        "\n",
        "  ```\n",
        "  sudo apt update\n",
        "  sudo apt install git\n",
        "  ```\n",
        "\n",
        "**2. Настройте имя пользователя и email.**\n",
        "\n",
        "Это нужно для подписи ваших коммитов, сделайте в терминале через `git config ...`.\n",
        "\n",
        "**3. Создайте аккаунт на GitHub**, если у вас его еще нет:\n",
        "[https://github.com](https://github.com)\n",
        "\n",
        "**4. Создайте новый репозиторий на GitHub:**\n",
        "\n",
        "* Найдите кнопку **New repository**.\n",
        "* Укажите название, краткое описание, выберите тип **Public** (чтобы мы могли проверить ДЗ).\n",
        "* Не ставьте галочку Initialize this repository with a README.\n",
        "\n",
        "**5. Создайте локальную папку с проектом.** Можно в терминале, можно через UI, это не имеет значения.\n",
        "\n",
        "**6. Инициализируйте Git в этой папке.** Здесь уже придется воспользоваться некоторой командой в терминале.\n",
        "\n",
        "**7. Привяжите локальный репозиторий к удаленному на GitHub.**\n",
        "\n",
        "**8. Создайте ветку разработки.** По умолчанию вы будете находиться в ветке `main`, создайте и переключитесь на ветку `hw-books-parser`.\n",
        "\n",
        "**9. Добавьте в проект следующие файлы и папки:**\n",
        "\n",
        "* `scraper.py` — ваш основной скрипт для сбора данных.\n",
        "* `README.md` — файл с кратким описанием проекта:\n",
        "\n",
        "  * цель;\n",
        "  * инструкции по запуску;\n",
        "  * список используемых библиотек.\n",
        "* `requirements.txt` — файл со списком зависимостей, необходимых для проекта (не присылайте все из глобального окружения, создайте изолированную виртуальную среду, добавьте в нее все нужное для проекта и получите список библиотек через `pip freeze`).\n",
        "* `artifacts/` — папка с результатами парсинга (`books_data.txt` — полностью или его часть, если весь не поместится на GitHub).\n",
        "* `notebooks/` — папка с заполненным ноутбуком `HW_03_python_ds_2025.ipynb` и запущенными ячейками с выводами на экран.\n",
        "* `tests/` — папка с тестами на `pytest`, оформите их в формате скрипта(-ов) с расширением `.py`.\n",
        "* `.gitignore` — стандартный файл, который позволит исключить временные файлы при добавлении в отслеживаемые (например, `__pycache__/`, `.DS_Store`, `*.pyc`, `venv/` и др.).\n",
        "\n",
        "\n",
        "**10. Сделайте коммит.**\n",
        "\n",
        "**11. Отправьте свою ветку на GitHub.**\n",
        "\n",
        "**12. Создайте Pull Request:**\n",
        "\n",
        "* Перейдите в репозиторий на GitHub.\n",
        "* Нажмите кнопку **Compare & pull request**.\n",
        "* Укажите, что было добавлено, и нажмите **Create pull request**.\n",
        "\n",
        "**13. Выполните слияние Pull Request:**\n",
        "\n",
        "* Убедитесь, что нет конфликтов.\n",
        "* Нажмите **Merge pull request**, затем **Confirm merge**.\n",
        "\n",
        "**14. Скачайте изменения из основной ветки локально.**\n",
        "\n",
        "\n",
        "\n",
        "### Требования к итоговому репозиторию\n",
        "\n",
        "* Файл `scraper.py` с рабочим кодом парсера.\n",
        "* `README.md` с описанием проекта и инструкцией по запуску.\n",
        "* Папка `artifacts/` с результатом сбора данных (`.txt` файл).\n",
        "* Папка `tests/` с тестами на `pytest`.\n",
        "* Папка `notebooks/` с заполненным ноутбуком `HW_03_python_ds_2025.ipynb`.\n",
        "* Pull Request с комментарием из ветки `hw-books-parser` в ветку `main`.\n",
        "* Примерная структура:\n",
        "\n",
        "  ```\n",
        "  books_scraper/\n",
        "  ├── artifacts/\n",
        "  │   └── books_data.txt\n",
        "  ├── notebooks/\n",
        "  │   └── HW_03_python_ds_2025.ipynb\n",
        "  ├── scraper.py\n",
        "  ├── README.md\n",
        "  ├── tests/\n",
        "  │   └── test_scraper.py\n",
        "  ├── .gitignore\n",
        "  └── requirements.txt\n",
        "  ```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
